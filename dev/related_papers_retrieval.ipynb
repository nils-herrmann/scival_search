{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90392b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scival_search import RelatedPapers\n",
    "\n",
    "env_vars = dotenv_values()\n",
    "COOKIE = env_vars['COOKIE']\n",
    "\n",
    "df = pd.read_csv(\"lookups.csv\", dtype={\"topic\": str})\n",
    "topics = list(set(df['topic'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b249d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output file\n",
    "output_file = \"related_papers.csv\"\n",
    "\n",
    "# Check which topics have already been processed\n",
    "processed_topics = set()\n",
    "if os.path.exists(output_file):\n",
    "    existing_df = pd.read_csv(output_file, dtype={\"Topic\": str})\n",
    "    processed_topics = set(existing_df['Topic'].unique())\n",
    "    print(f\"Found {len(processed_topics)} already processed topics\")\n",
    "else:\n",
    "    # Create empty CSV with headers\n",
    "    pd.DataFrame(columns=[\"EID\", \"Year\", \"Topic\"]).to_csv(output_file, index=False)\n",
    "    print(\"Created new output file\")\n",
    "\n",
    "# Filter out already processed topics\n",
    "topics_to_process = [t for t in topics if t not in processed_topics]\n",
    "print(f\"Total topics: {len(topics)}\")\n",
    "print(f\"Already processed: {len(processed_topics)}\")\n",
    "print(f\"To process: {len(topics_to_process)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve related papers for each topic and append incrementally\n",
    "failed_topics_file = \"failed_topics.txt\"\n",
    "\n",
    "for topic in tqdm(topics_to_process, desc=\"Topics\"):\n",
    "    try:\n",
    "        related_papers = RelatedPapers(topic, cookie=COOKIE, show_progress=False)\n",
    "        results = related_papers.results\n",
    "        results[\"Topic\"] = topic\n",
    "        \n",
    "        # Append to CSV immediately (one topic at a time)\n",
    "        results[[\"EID\", \"Year\", \"Topic\"]].to_csv(\n",
    "            output_file, \n",
    "            mode='a',  # Append mode\n",
    "            header=False,  # Don't write headers again\n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Log the failed topic ID\n",
    "        with open(failed_topics_file, 'a') as f:\n",
    "            f.write(f\"{topic}\\n\")\n",
    "        print(f\"\\nFailed to process topic {topic}: {e}\")\n",
    "        # Continue with next topic \n",
    "\n",
    "print(f\"\\nCompleted! Results saved to {output_file}\")\n",
    "if os.path.exists(failed_topics_file):\n",
    "    with open(failed_topics_file, 'r') as f:\n",
    "        failed_count = len(f.readlines())\n",
    "    print(f\"Failed topics: {failed_count} (saved to {failed_topics_file})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check for duplicates and missing topics\n",
    "import pandas as pd\n",
    "\n",
    "df_todo = pd.read_csv(\"lookups.csv\", dtype={\"topic\": str})\n",
    "topics = list(set(df_todo['topic'].tolist()))\n",
    "\n",
    "# Output file\n",
    "output_file = \"related_papers.csv\"\n",
    "df_processed = pd.read_csv(output_file, dtype={\"Topic\": str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfe283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df_processed[df_processed.duplicated(subset=['EID', 'Topic'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(f\"Found {len(duplicates)} duplicate entries:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicate entries found.\")\n",
    "\n",
    "# Check for missing topics\n",
    "processed_topics = set(df_processed['Topic'].unique())\n",
    "missing_topics = [t for t in topics if t not in processed_topics]\n",
    "if missing_topics:\n",
    "    print(f\"Missing topics ({len(missing_topics)}): {missing_topics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scival_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
